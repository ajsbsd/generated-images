{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!pip install diffusers transformers accelerate bitsandbytes Pillow requests torch\n",
    "\"\"\"\n",
    "# Stable Diffusion Image Generation with NSFW Detection (Local)\n",
    "\n",
    "This script provides a comprehensive solution for generating images using Stable Diffusion\n",
    "on a local machine, incorporating a post-processing NSFW detection step.\n",
    "\n",
    "Key Features:\n",
    "- **Stable Diffusion Image Generation**: Utilizes `diffusers` to create images from prompts.\n",
    "- **Configurable Parameters**: Easily adjust prompts, negative prompts, steps, and guidance scale.\n",
    "- **Metadata Embedding**: Saves generated images as PNGs with embedded generation parameters.\n",
    "- **NSFW Detection**: Includes a placeholder for a CLIP-based NSFW detector\n",
    "  to filter out undesirable content after generation. Images flagged as NSFW are\n",
    "  quarantined to a separate directory. The detection model is only loaded if a CUDA GPU is available.\n",
    "\n",
    "**Important Note on NSFW Detection:**\n",
    "The NSFW detection implemented here is **conceptual by default**. For a robust solution,\n",
    "you'll need to uncomment and configure the actual CLIP model loading.\n",
    "The placeholder `is_nsfw` function simulates detection based on a random chance\n",
    "or keyword presence if the actual CLIP model is not loaded.\n",
    "\"\"\"\n",
    "\n",
    "# === Standard Library Imports ===\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import torch\n",
    "from PIL import Image, PngImagePlugin\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
    "\n",
    "# === NSFW Detection Libraries ===\n",
    "# IMPORTANT: Uncomment these imports if you want to use a real CLIP-based NSFW detector.\n",
    "# You will also need to install the 'transformers' library:\n",
    "# pip install transformers\n",
    "import torch.nn.functional as F # <-- UNCOMMENT THIS LINE\n",
    "from transformers import CLIPProcessor, CLIPModel # <-- UNCOMMENT THIS LINE\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "NSFW_QUARANTINE_DIR = \"nsfw_quarantined_images\" # Directory for flagged images\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(NSFW_QUARANTINE_DIR, exist_ok=True) # Create quarantine directory\n",
    "\n",
    "# Set a reasonable number of images to generate\n",
    "NUM_IMAGES_TO_GENERATE = 5\n",
    "\n",
    "BASE_PROMPT = \"Romanian Brown Bear\"\n",
    "NEGATIVE_PROMPT = (\n",
    "    \"(nsfw:1.5), (easynegative:1.3) (bad_prompt:1.3) badhandv4 bad-hands-5 (negative_hand-neg) \"\n",
    "    \"(bad-picture-chill-75v), (worst quality:1.3), (low quality:1.3), (bad quality:1.3), \"\n",
    "    \"(a shadow on skin:1.3), (a shaded skin:1.3), (a dark skin:1.3), (blush:1.3), \"\n",
    "    \"(signature, watermark, username, letter, copyright name, copyright, chinese text, artist name, name tag, \"\n",
    "    \"company name, name tag, text, error:1.5), (bad anatomy:1.5), (low quality hand:1.5), (worst quality hand:1.5)\"\n",
    ")\n",
    "\n",
    "GENERATION_CONFIG = {\n",
    "    \"vae\": \"stabilityai/sd-vae-ft-mse\",\n",
    "    \"sampler\": \"Euler a\", # Note: Diffusers pipeline uses `scheduler` parameter for sampler\n",
    "    \"steps\": 25,\n",
    "    \"guidance_scale\": 7.0\n",
    "}\n",
    "\n",
    "# === Initialize Model ===\n",
    "# Determine the device for model execution\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Determine the dtype based on the device\n",
    "# Use float16 for CUDA (GPU) for performance, else use float32 for CPU compatibility\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "print(f\"Using dtype: {DTYPE}\")\n",
    "\n",
    "# Load the VAE model separately if specified\n",
    "vae = None\n",
    "if \"vae\" in GENERATION_CONFIG and GENERATION_CONFIG[\"vae\"]:\n",
    "    try:\n",
    "        vae = AutoencoderKL.from_pretrained(GENERATION_CONFIG[\"vae\"], torch_dtype=DTYPE).to(DEVICE)\n",
    "        print(f\"Loaded VAE: {GENERATION_CONFIG['vae']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load VAE {GENERATION_CONFIG['vae']}. Error: {e}. Proceeding without it.\")\n",
    "        vae = None\n",
    "## 05.24.2025 - ajsbsd.net python dev library\n",
    "\n",
    "## 11.01.2023 - ajsbsd.net website with Hugging Face Chat inference\n",
    "\n",
    "[Live at ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "## Tools\n",
    "\n",
    "- [Debian/bookworm](https://debian.org)\n",
    "- [Apache](https://httpd.apache.org)\n",
    "- [NextJS 14.0.1 now running in Docker](https://nextjs.org)\n",
    "- [TailwindCSS](https://tailwindcss.com/docs/customizing-colors)\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Interact with HF API similar to [Vercel Hugging Face Guide](https://sdk.vercel.ai/docs/guides/providers/hugging-face)\n",
    "- Learn how to make UI look tolerable without Radix-UI Theme\n",
    "- Experiment with HF Inference since my server won't run OpenLLM/BentoML very well :)\n",
    "\n",
    "## Docker\n",
    "\n",
    "### 11.28.2023 - Dockerfile live at [ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "```bash\n",
    "$ docker build -t ajsbsd.net-nextjs-docker .\n",
    "$ docker run -p 127.0.0.1:3000:3000 ajsbsd.net-nextjs-docker\n",
    "\n",
    "# Initialize the Stable Diffusion pipeline\n",
    "# Pass DTYPE to torch_dtype for dynamic precision\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=DTYPE, vae=vae)\n",
    "pipe.to(DEVICE)\n",
    "# Enable attention slicing for lower VRAM usage, only if on CUDA\n",
    "if DEVICE == \"cuda\":\n",
    "    pipe.enable_attention_slicing()\n",
    "    print(\"Attention slicing enabled for CUDA device.\")\n",
    "\n",
    "# === Initialize NSFW Detector (Conceptual/Optional Real) ===\n",
    "# IMPORTANT: Uncomment the following lines if you want to use a real CLIP-based NSFW detector.\n",
    "# Using 'laion/CLIP-ViT-B-32-laion2B-s34B-b79K' is a good starting point.\n",
    "nsfw_detector_model = None\n",
    "nsfw_detector_processor = None\n",
    "try:\n",
    "    if DEVICE == \"cpu\":\n",
    "        print(\"NSFW Detector: Running on CPU, skipping full CLIP model load for performance.\")\n",
    "    else:\n",
    "        print(\"Attempting to load CLIP-based NSFW detector...\")\n",
    "        nsfw_detector_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\").to(DEVICE) # <-- THIS LINE WILL NOW WORK\n",
    "        nsfw_detector_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\") # <-- THIS LINE WILL NOW WORK\n",
    "        print(\"âœ… CLIP-based NSFW Detector initialized.\")\n",
    "except ImportError:\n",
    "    print(\"âŒ 'transformers' library not found. Please install it for real NSFW detection.\")\n",
    "    print(\"   `pip install transformers`\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not initialize CLIP-based NSFW Detector. Error: {e}. NSFW detection will be conceptual.\")\n",
    "\n",
    "# Placeholder for conceptual NSFW detection if real one isn't loaded/fails\n",
    "# This line is now effectively the fallback if the above 'try' block fails\n",
    "# nsfw_detector_model = None\n",
    "# nsfw_detector_processor = None\n",
    "# print(\"NSFW detection will be conceptual unless CLIP model loading is uncommented and successful.\")\n",
    "\n",
    "\n",
    "# --- Functions ---\n",
    "\n",
    "def add_metadata_and_save(image: Image.Image, filepath: str, prompt: str, negative_prompt: str, seed: int):\n",
    "    \"\"\"\n",
    "    Embeds generation metadata into a PNG image and saves it to the specified filepath.\n",
    "    \"\"\"\n",
    "    meta = PngImagePlugin.PngInfo()\n",
    "    meta.add_text(\"Prompt\", prompt)\n",
    "    meta.add_text(\"NegativePrompt\", negative_prompt)\n",
    "    meta.add_text(\"Model\", MODEL_ID)\n",
    "    meta.add_text(\"VAE\", GENERATION_CONFIG.get(\"vae\", \"N/A\"))\n",
    "    meta.add_text(\"Sampler\", GENERATION_CONFIG.get(\"sampler\", \"N/A\"))\n",
    "    meta.add_text(\"Steps\", str(GENERATION_CONFIG.get(\"steps\", \"N/A\")))\n",
    "    meta.add_text(\"Seed\", str(seed))\n",
    "    meta.add_text(\"Date\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    image.save(filepath, \"PNG\", pnginfo=meta)\n",
    "    print(f\"Metadata added and image saved to: {filepath}\")\n",
    "# -*- coding: utf-8 -*-\n",
    "#!pip install diffusers transformers accelerate bitsandbytes Pillow requests torch\n",
    "\"\"\"\n",
    "# Stable Diffusion Image Generation with NSFW Detection (Local)\n",
    "\n",
    "This script provides a comprehensive solution for generating images using Stable Diffusion\n",
    "on a local machine, incorporating a post-processing NSFW detection step.\n",
    "\n",
    "Key Features:\n",
    "- **Stable Diffusion Image Generation**: Utilizes `diffusers` to create images from prompts.\n",
    "- **Configurable Parameters**: Easily adjust prompts, negative prompts, steps, and guidance scale.\n",
    "- **Metadata Embedding**: Saves generated images as PNGs with embedded generation parameters.\n",
    "- **NSFW Detection**: Includes a placeholder for a CLIP-based NSFW detector\n",
    "  to filter out undesirable content after generation. Images flagged as NSFW are\n",
    "  quarantined to a separate directory. The detection model is only loaded if a CUDA GPU is available.\n",
    "\n",
    "**Important Note on NSFW Detection:**\n",
    "The NSFW detection implemented here is **conceptual by default**. For a robust solution,\n",
    "you'll need to uncomment and configure the actual CLIP model loading.\n",
    "The placeholder `is_nsfw` function simulates detection based on a random chance\n",
    "or keyword presence if the actual CLIP model is not loaded.\n",
    "\"\"\"\n",
    "\n",
    "# === Standard Library Imports ===\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import torch\n",
    "from PIL import Image, PngImagePlugin\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
    "\n",
    "# === NSFW Detection Libraries ===\n",
    "# IMPORTANT: Uncomment these imports if you want to use a real CLIP-based NSFW detector.\n",
    "# You will also need to install the 'transformers' library:\n",
    "# pip install transformers\n",
    "import torch.nn.functional as F # <-- UNCOMMENT THIS LINE\n",
    "from transformers import CLIPProcessor, CLIPModel # <-- UNCOMMENT THIS LINE\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "NSFW_QUARANTINE_DIR = \"nsfw_quarantined_images\" # Directory for flagged images\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(NSFW_QUARANTINE_DIR, exist_ok=True) # Create quarantine directory\n",
    "\n",
    "# Set a reasonable number of images to generate\n",
    "NUM_IMAGES_TO_GENERATE = 5\n",
    "\n",
    "BASE_PROMPT = \"Romanian Brown Bear\"\n",
    "NEGATIVE_PROMPT = (\n",
    "    \"(nsfw:1.5), (easynegative:1.3) (bad_prompt:1.3) badhandv4 bad-hands-5 (negative_hand-neg) \"\n",
    "    \"(bad-picture-chill-75v), (worst quality:1.3), (low quality:1.3), (bad quality:1.3), \"\n",
    "    \"(a shadow on skin:1.3), (a shaded skin:1.3), (a dark skin:1.3), (blush:1.3), \"\n",
    "    \"(signature, watermark, username, letter, copyright name, copyright, chinese text, artist name, name tag, \"\n",
    "    \"company name, name tag, text, error:1.5), (bad anatomy:1.5), (low quality hand:1.5), (worst quality hand:1.5)\"\n",
    ")\n",
    "\n",
    "GENERATION_CONFIG = {\n",
    "    \"vae\": \"stabilityai/sd-vae-ft-mse\",\n",
    "    \"sampler\": \"Euler a\", # Note: Diffusers pipeline uses `scheduler` parameter for sampler\n",
    "    \"steps\": 25,\n",
    "    \"guidance_scale\": 7.0\n",
    "}\n",
    "\n",
    "# === Initialize Model ===\n",
    "# Determine the device for model execution\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Determine the dtype based on the device\n",
    "# Use float16 for CUDA (GPU) for performance, else use float32 for CPU compatibility\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "print(f\"Using dtype: {DTYPE}\")\n",
    "\n",
    "# Load the VAE model separately if specified\n",
    "vae = None\n",
    "if \"vae\" in GENERATION_CONFIG and GENERATION_CONFIG[\"vae\"]:\n",
    "    try:\n",
    "        vae = AutoencoderKL.from_pretrained(GENERATION_CONFIG[\"vae\"], torch_dtype=DTYPE).to(DEVICE)\n",
    "        print(f\"Loaded VAE: {GENERATION_CONFIG['vae']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load VAE {GENERATION_CONFIG['vae']}. Error: {e}. Proceeding without it.\")\n",
    "        vae = None\n",
    "## 05.24.2025 - ajsbsd.net python dev library\n",
    "\n",
    "## 11.01.2023 - ajsbsd.net website with Hugging Face Chat inference\n",
    "\n",
    "[Live at ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "## Tools\n",
    "\n",
    "- [Debian/bookworm](https://debian.org)\n",
    "- [Apache](https://httpd.apache.org)\n",
    "- [NextJS 14.0.1 now running in Docker](https://nextjs.org)\n",
    "- [TailwindCSS](https://tailwindcss.com/docs/customizing-colors)\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Interact with HF API similar to [Vercel Hugging Face Guide](https://sdk.vercel.ai/docs/guides/providers/hugging-face)\n",
    "- Learn how to make UI look tolerable without Radix-UI Theme\n",
    "- Experiment with HF Inference since my server won't run OpenLLM/BentoML very well :)\n",
    "\n",
    "## Docker\n",
    "\n",
    "### 11.28.2023 - Dockerfile live at [ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "```bash\n",
    "$ docker build -t ajsbsd.net-nextjs-docker .\n",
    "$ docker run -p 127.0.0.1:3000:3000 ajsbsd.net-nextjs-docker\n",
    "\n",
    "# Initialize the Stable Diffusion pipeline\n",
    "# Pass DTYPE to torch_dtype for dynamic precision\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=DTYPE, vae=vae)\n",
    "pipe.to(DEVICE)\n",
    "# Enable attention slicing for lower VRAM usage, only if on CUDA\n",
    "if DEVICE == \"cuda\":\n",
    "    pipe.enable_attention_slicing()\n",
    "    print(\"Attention slicing enabled for CUDA device.\")\n",
    "\n",
    "# === Initialize NSFW Detector (Conceptual/Optional Real) ===\n",
    "# IMPORTANT: Uncomment the following lines if you want to use a real CLIP-based NSFW detector.\n",
    "# Using 'laion/CLIP-ViT-B-32-laion2B-s34B-b79K' is a good starting point.\n",
    "nsfw_detector_model = None\n",
    "nsfw_detector_processor = None\n",
    "try:\n",
    "    if DEVICE == \"cpu\":\n",
    "        print(\"NSFW Detector: Running on CPU, skipping full CLIP model load for performance.\")\n",
    "    else:\n",
    "        print(\"Attempting to load CLIP-based NSFW detector...\")\n",
    "        nsfw_detector_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\").to(DEVICE) # <-- THIS LINE WILL NOW WORK\n",
    "        nsfw_detector_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\") # <-- THIS LINE WILL NOW WORK\n",
    "        print(\"âœ… CLIP-based NSFW Detector initialized.\")\n",
    "except ImportError:\n",
    "    print(\"âŒ 'transformers' library not found. Please install it for real NSFW detection.\")\n",
    "    print(\"   `pip install transformers`\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not initialize CLIP-based NSFW Detector. Error: {e}. NSFW detection will be conceptual.\")\n",
    "\n",
    "# Placeholder for conceptual NSFW detection if real one isn't loaded/fails\n",
    "# This line is now effectively the fallback if the above 'try' block fails\n",
    "# nsfw_detector_model = None\n",
    "# nsfw_detector_processor = None\n",
    "# print(\"NSFW detection will be conceptual unless CLIP model loading is uncommented and successful.\")\n",
    "\n",
    "\n",
    "# --- Functions ---\n",
    "\n",
    "def add_metadata_and_save(image: Image.Image, filepath: str, prompt: str, negative_prompt: str, seed: int):\n",
    "    \"\"\"\n",
    "    Embeds generation metadata into a PNG image and saves it to the specified filepath.\n",
    "    \"\"\"\n",
    "    meta = PngImagePlugin.PngInfo()\n",
    "    meta.add_text(\"Prompt\", prompt)\n",
    "    meta.add_text(\"NegativePrompt\", negative_prompt)\n",
    "    meta.add_text(\"Model\", MODEL_ID)\n",
    "    meta.add_text(\"VAE\", GENERATION_CONFIG.get(\"vae\", \"N/A\"))\n",
    "    meta.add_text(\"Sampler\", GENERATION_CONFIG.get(\"sampler\", \"N/A\"))\n",
    "    meta.add_text(\"Steps\", str(GENERATION_CONFIG.get(\"steps\", \"N/A\")))\n",
    "    meta.add_text(\"Seed\", str(seed))\n",
    "    meta.add_text(\"Date\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    image.save(filepath, \"PNG\", pnginfo=meta)\n",
    "    print(f\"Metadata added and image saved to: {filepath}\")\n",
    "\n",
    "\n",
    "def is_nsfw(image: Image.Image) -> tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Detects NSFW content.\n",
    "    If a real CLIP-based detector is loaded, it will be used.\n",
    "    Otherwise, it falls back to a conceptual, random detection.\n",
    "    \"\"\"\n",
    "    if nsfw_detector_model is not None and nsfw_detector_processor is not None:\n",
    "        # --- START OF REAL CLIP-BASED NSFW DETECTION (if uncommented and loaded) ---\n",
    "        # Ensure F (torch.nn.functional) is imported if using F.normalize\n",
    "        # from transformers import CLIPProcessor, CLIPModel\n",
    "        # import torch.nn.functional as F\n",
    "\n",
    "        inputs = nsfw_detector_processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "        image_features = nsfw_detector_model.get_image_features(**inputs)\n",
    "\n",
    "        safe_text_prompts = [\"a photo of a person\", \"a general image\", \"a family-friendly photo\"]\n",
    "        unsafe_text_prompts = [\"a photo of nudity\", \"a pornographic image\", \"an offensive image\", \"explicit content\"]\n",
    "\n",
    "        text_inputs_safe = nsfw_detector_processor(text=safe_text_prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        text_features_safe = nsfw_detector_model.get_text_features(**text_inputs_safe)\n",
    "\n",
    "        text_inputs_unsafe = nsfw_detector_processor(text=unsafe_text_prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        text_features_unsafe = nsfw_detector_model.get_text_features(**text_inputs_unsafe)\n",
    "\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features_safe = F.normalize(text_features_safe, p=2, dim=-1)\n",
    "        text_features_unsafe = F.normalize(text_features_unsafe, p=2, dim=-1)\n",
    "## 05.24.2025 - ajsbsd.net python dev library\n",
    "\n",
    "## 11.01.2023 - ajsbsd.net website with Hugging Face Chat inference\n",
    "\n",
    "[Live at ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "## Tools\n",
    "\n",
    "- [Debian/bookworm](https://debian.org)\n",
    "- [Apache](https://httpd.apache.org)\n",
    "- [NextJS 14.0.1 now running in Docker](https://nextjs.org)\n",
    "- [TailwindCSS](https://tailwindcss.com/docs/customizing-colors)\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Interact with HF API similar to [Vercel Hugging Face Guide](https://sdk.vercel.ai/docs/guides/providers/hugging-face)\n",
    "- Learn how to make UI look tolerable without Radix-UI Theme\n",
    "- Experiment with HF Inference since my server won't run OpenLLM/BentoML very well :)\n",
    "\n",
    "## Docker\n",
    "\n",
    "### 11.28.2023 - Dockerfile live at [ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "```bash\n",
    "$ docker build -t ajsbsd.net-nextjs-docker .\n",
    "$ docker run -p 127.0.0.1:3000:3000 ajsbsd.net-nextjs-docker\n",
    "\n",
    "        safe_similarity = (image_features @ text_features_safe.T).mean().item()\n",
    "        unsafe_similarity = (image_features @ text_features_unsafe.T).mean().item()\n",
    "\n",
    "        # Simple classification based on which similarity is higher and a threshold\n",
    "        nsfw_threshold = 0.25 # Adjust based on experimentation\n",
    "        if unsafe_similarity > safe_similarity and unsafe_similarity > nsfw_threshold:\n",
    "            print(f\"  ðŸš¨ Detected as potentially NSFW (CLIP-based: Unsafe {unsafe_similarity:.2f} > Safe {safe_similarity:.2f}).\")\n",
    "            return True, unsafe_similarity\n",
    "        else:\n",
    "            return False, unsafe_similarity\n",
    "        # --- END OF REAL CLIP-BASED NSFW DETECTION ---\n",
    "\n",
    "    # --- START OF CONCEPTUAL/FALLBACK NSFW DETECTION LOGIC ---\n",
    "    # This runs if the real CLIP model is not loaded or if the above real logic isn't fully implemented.\n",
    "    if \"nude\" in BASE_PROMPT.lower() or random.random() < 0.15: # 15% chance to be flagged NSFW for demo\n",
    "        print(\"  ðŸš¨ Detected as potentially NSFW (conceptual detection).\")\n",
    "        return True, random.uniform(0.6, 0.98) # Simulate high confidence\n",
    "    else:\n",
    "        return False, random.uniform(0.01, 0.3) # Simulate low confidence\n",
    "    # --- END OF CONCEPTUAL/FALLBACK NSFW DETECTION LOGIC ---\n",
    "\n",
    "\n",
    "def generate_and_process_images(num_images: int = 1):\n",
    "    \"\"\"\n",
    "    Generates a specified number of images, performs NSFW detection,\n",
    "    and saves them with metadata.\n",
    "    \"\"\"\n",
    "    for i in range(num_images):\n",
    "        variation = \", vibrant colors, neon lights\" if i % 2 == 0 else \", soft pastel tones, morning light\"\n",
    "        current_prompt = BASE_PROMPT + variation\n",
    "        seed = random.randint(10000000, 99999999)\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "\n",
    "        print(f\"\\n--- Generating image {i + 1}/{num_images} with seed {seed} ---\")\n",
    "        print(f\"Prompt: {current_prompt}\")\n",
    "\n",
    "        try:\n",
    "            result = pipe(\n",
    "                prompt=current_prompt,\n",
    "                negative_prompt=NEGATIVE_PROMPT,\n",
    "                num_inference_steps=GENERATION_CONFIG[\"steps\"],\n",
    "                guidance_scale=GENERATION_CONFIG[\"guidance_scale\"],\n",
    "                generator=generator,\n",
    "            )\n",
    "            image = result.images[0]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Image generation failed for image {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- NSFW Detection Step ---\n",
    "        is_flagged, confidence = is_nsfw(image)\n",
    "        if is_flagged:\n",
    "            print(f\"!!! Image flagged as NSFW (confidence: {confidence:.2f}). Moving to quarantine. !!!\")\n",
    "            filename = os.path.join(NSFW_QUARANTINE_DIR, f\"nsfw_image_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{i}.png\")\n",
    "            add_metadata_and_save(image, filename, current_prompt, NEGATIVE_PROMPT, seed)\n",
    "            print(f\"Quarantined: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Image passed NSFW check (confidence: {confidence:.2f}). Proceeding to save.\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        filename = os.path.join(OUTPUT_DIR, f\"image_{timestamp}_{i}.png\")\n",
    "\n",
    "        add_metadata_and_save(image, filename, current_prompt, NEGATIVE_PROMPT, seed)\n",
    "\n",
    "\n",
    "# === Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Starting Stable Diffusion Image Generation ---\")\n",
    "    try:\n",
    "        generate_and_process_images(num_images=NUM_IMAGES_TO_GENERATE)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the process: {e}\")\n",
    "    finally:\n",
    "        # Clean up GPU memory\n",
    "        del pipe\n",
    "        if 'vae' in locals() and vae is not None:\n",
    "            del vae\n",
    "        # Uncomment these lines if you enable and load the real NSFW detector\n",
    "        if 'nsfw_detector_model' in locals() and nsfw_detector_model is not None: # <-- UNCOMMENT THIS LINE\n",
    "            del nsfw_detector_model # <-- UNCOMMENT THIS LINE\n",
    "        if 'nsfw_detector_processor' in locals() and nsfw_detector_processor is not None: # <-- UNCOMMENT THIS LINE\n",
    "            del nsfw_detector_processor # <-- UNCOMMENT THIS LINE\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\n--- Process finished. GPU memory cleared. ---\")\n",
    "        print(f\"Generated images saved to: {OUTPUT_DIR}\")\n",
    "        print(f\"NSFW flagged images (if any) saved to: {NSFW_QUARANTINE_DIR}\")\n",
    "\n",
    "def is_nsfw(image: Image.Image) -> tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Detects NSFW content.\n",
    "    If a real CLIP-based detector is loaded, it will be used.\n",
    "    Otherwise, it falls back to a conceptual, random detection.\n",
    "    \"\"\"\n",
    "    if nsfw_detector_model is not None and nsfw_detector_processor is not None:\n",
    "        # --- START OF REAL CLIP-BASED NSFW DETECTION (if uncommented and loaded) ---\n",
    "        # Ensure F (torch.nn.functional) is imported if using F.normalize\n",
    "        # from transformers import CLIPProcessor, CLIPModel\n",
    "        # import torch.nn.functional as F\n",
    "\n",
    "        inputs = nsfw_detector_processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "        image_features = nsfw_detector_model.get_image_features(**inputs)\n",
    "\n",
    "        safe_text_prompts = [\"a photo of a person\", \"a general image\", \"a family-friendly photo\"]\n",
    "        unsafe_text_prompts = [\"a photo of nudity\", \"a pornographic image\", \"an offensive image\", \"explicit content\"]\n",
    "\n",
    "        text_inputs_safe = nsfw_detector_processor(text=safe_text_prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        text_features_safe = nsfw_detector_model.get_text_features(**text_inputs_safe)\n",
    "\n",
    "        text_inputs_unsafe = nsfw_detector_processor(text=unsafe_text_prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        text_features_unsafe = nsfw_detector_model.get_text_features(**text_inputs_unsafe)\n",
    "\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features_safe = F.normalize(text_features_safe, p=2, dim=-1)\n",
    "        text_features_unsafe = F.normalize(text_features_unsafe, p=2, dim=-1)\n",
    "## 05.24.2025 - ajsbsd.net python dev library\n",
    "\n",
    "## 11.01.2023 - ajsbsd.net website with Hugging Face Chat inference\n",
    "\n",
    "[Live at ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "## Tools\n",
    "\n",
    "- [Debian/bookworm](https://debian.org)\n",
    "- [Apache](https://httpd.apache.org)\n",
    "- [NextJS 14.0.1 now running in Docker](https://nextjs.org)\n",
    "- [TailwindCSS](https://tailwindcss.com/docs/customizing-colors)\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Interact with HF API similar to [Vercel Hugging Face Guide](https://sdk.vercel.ai/docs/guides/providers/hugging-face)\n",
    "- Learn how to make UI look tolerable without Radix-UI Theme\n",
    "- Experiment with HF Inference since my server won't run OpenLLM/BentoML very well :)\n",
    "\n",
    "## Docker\n",
    "\n",
    "### 11.28.2023 - Dockerfile live at [ajsbsd.net](https://ajsbsd.net)\n",
    "\n",
    "```bash\n",
    "$ docker build -t ajsbsd.net-nextjs-docker .\n",
    "$ docker run -p 127.0.0.1:3000:3000 ajsbsd.net-nextjs-docker\n",
    "\n",
    "        safe_similarity = (image_features @ text_features_safe.T).mean().item()\n",
    "        unsafe_similarity = (image_features @ text_features_unsafe.T).mean().item()\n",
    "\n",
    "        # Simple classification based on which similarity is higher and a threshold\n",
    "        nsfw_threshold = 0.25 # Adjust based on experimentation\n",
    "        if unsafe_similarity > safe_similarity and unsafe_similarity > nsfw_threshold:\n",
    "            print(f\"  ðŸš¨ Detected as potentially NSFW (CLIP-based: Unsafe {unsafe_similarity:.2f} > Safe {safe_similarity:.2f}).\")\n",
    "            return True, unsafe_similarity\n",
    "        else:\n",
    "            return False, unsafe_similarity\n",
    "        # --- END OF REAL CLIP-BASED NSFW DETECTION ---\n",
    "\n",
    "    # --- START OF CONCEPTUAL/FALLBACK NSFW DETECTION LOGIC ---\n",
    "    # This runs if the real CLIP model is not loaded or if the above real logic isn't fully implemented.\n",
    "    if \"nude\" in BASE_PROMPT.lower() or random.random() < 0.15: # 15% chance to be flagged NSFW for demo\n",
    "        print(\"  ðŸš¨ Detected as potentially NSFW (conceptual detection).\")\n",
    "        return True, random.uniform(0.6, 0.98) # Simulate high confidence\n",
    "    else:\n",
    "        return False, random.uniform(0.01, 0.3) # Simulate low confidence\n",
    "    # --- END OF CONCEPTUAL/FALLBACK NSFW DETECTION LOGIC ---\n",
    "\n",
    "\n",
    "def generate_and_process_images(num_images: int = 1):\n",
    "    \"\"\"\n",
    "    Generates a specified number of images, performs NSFW detection,\n",
    "    and saves them with metadata.\n",
    "    \"\"\"\n",
    "    for i in range(num_images):\n",
    "        variation = \", vibrant colors, neon lights\" if i % 2 == 0 else \", soft pastel tones, morning light\"\n",
    "        current_prompt = BASE_PROMPT + variation\n",
    "        seed = random.randint(10000000, 99999999)\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "\n",
    "        print(f\"\\n--- Generating image {i + 1}/{num_images} with seed {seed} ---\")\n",
    "        print(f\"Prompt: {current_prompt}\")\n",
    "\n",
    "        try:\n",
    "            result = pipe(\n",
    "                prompt=current_prompt,\n",
    "                negative_prompt=NEGATIVE_PROMPT,\n",
    "                num_inference_steps=GENERATION_CONFIG[\"steps\"],\n",
    "                guidance_scale=GENERATION_CONFIG[\"guidance_scale\"],\n",
    "                generator=generator,\n",
    "            )\n",
    "            image = result.images[0]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Image generation failed for image {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- NSFW Detection Step ---\n",
    "        is_flagged, confidence = is_nsfw(image)\n",
    "        if is_flagged:\n",
    "            print(f\"!!! Image flagged as NSFW (confidence: {confidence:.2f}). Moving to quarantine. !!!\")\n",
    "            filename = os.path.join(NSFW_QUARANTINE_DIR, f\"nsfw_image_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{i}.png\")\n",
    "            add_metadata_and_save(image, filename, current_prompt, NEGATIVE_PROMPT, seed)\n",
    "            print(f\"Quarantined: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Image passed NSFW check (confidence: {confidence:.2f}). Proceeding to save.\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        filename = os.path.join(OUTPUT_DIR, f\"image_{timestamp}_{i}.png\")\n",
    "\n",
    "        add_metadata_and_save(image, filename, current_prompt, NEGATIVE_PROMPT, seed)\n",
    "\n",
    "\n",
    "# === Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Starting Stable Diffusion Image Generation ---\")\n",
    "    try:\n",
    "        generate_and_process_images(num_images=NUM_IMAGES_TO_GENERATE)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the process: {e}\")\n",
    "    finally:\n",
    "        # Clean up GPU memory\n",
    "        del pipe\n",
    "        if 'vae' in locals() and vae is not None:\n",
    "            del vae\n",
    "        # Uncomment these lines if you enable and load the real NSFW detector\n",
    "        if 'nsfw_detector_model' in locals() and nsfw_detector_model is not None: # <-- UNCOMMENT THIS LINE\n",
    "            del nsfw_detector_model # <-- UNCOMMENT THIS LINE\n",
    "        if 'nsfw_detector_processor' in locals() and nsfw_detector_processor is not None: # <-- UNCOMMENT THIS LINE\n",
    "            del nsfw_detector_processor # <-- UNCOMMENT THIS LINE\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\n--- Process finished. GPU memory cleared. ---\")\n",
    "        print(f\"Generated images saved to: {OUTPUT_DIR}\")\n",
    "        print(f\"NSFW flagged images (if any) saved to: {NSFW_QUARANTINE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
